### Project: The prediction of exercise quality: how well did you exercise? 

### Overview
In this project, I used data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. My primary goal is to predict their exercise quality, a categorical variable named "class" in the dataset.

### Data Analysis

#### Step 1. load the data and replace all the missing values as NAs. 

```{r, echo=TRUE}
training<-read.csv("pml-training.csv", header=TRUE,sep=",", na.strings=c("", "NA", "#DIV/0!"))
testing<-read.csv("pml-testing.csv", header=TRUE,sep=",", na.strings=c("", "NA", "#DIV/0!"))
```

#### Step 2: Split the training dataset into two parts: a trainingSub dataset and a validtion dataset, for cross validation purpose. 

```{r, echo=TRUE}
library(caret)
inTrain<-createDataPartition(y=training$class, p=0.75,list=FALSE)
trainingSub<-training[inTrain,]
validation<-training[-inTrain,]
```

#### Step 3. Let's examine the trainingSub dataset. 
```{r, echo=TRUE}
beltVarN<-sum(grepl("_belt", names(trainingSub)))
armVarN<-sum(grepl("_arm", names(trainingSub)))
forearmVarN<-sum(grepl("_forearm", names(trainingSub)))
dumbbellVarN<-sum(grepl("_dumbbell", names(trainingSub)))
featureVarN<-sum(beltVarN, armVarN, forearmVarN, dumbbellVarN)
```

There are 160 variables in total. Among those, 152 variabls are feature variables: 38 variables are arm-relevant variables; 38 are belt-relevant variables; 38 are forarm-relevant variables; and 38 are dumbbell-relevant variables. The remaining 8 variables include our outcome "class" variable, the variable that we plan to predict, and the other 7 identification variables such as X (sequence), user_name (a participant's name), and five time variables. I intend to use feature variables to predict outcome variable in this project. 

#### Step 4. clean up the trainingSub dataset.
In this step, I eliminated variables that have more than 97.5% of NA values. Please note 97.5% is subjective as I do not want to throw away a lot of data.  

```{r, echo=TRUE}
numberNA = NULL
for (i in 1:160) {
        numberNA[i]<-sum(is.na(trainingSub[,i]))
        numberNA
}
df<-data.frame(numberNA)
df$col<-rownames(df)
selVar<-df[df$numberNA<dim(trainingSub)[1]*0.975, ]
trainingSubNew<-trainingSub[,as.numeric(selVar$col)]
```

I started with 160 variables in total for my trainingSub dataset. After running the above procedure, I ended up with 60 variables now. Among these, only 52 of them are feature variables. 

#### Step 5. Fit the model on the trainingSub set. 

```{r, echo=TRUE}
modFit<-train(trainingSubNew$class~., method="rpart", data=trainingSubNew)
library(rattle)
fancyRpartPlot(modFit$finalModel)
```

I used tree method to predict the outcome "class" variable, because the "class"" variable is categorical and have 5 levels. Hence, I prefer the tree method over logistic regression.  


#### Step 6. Use the validation dataset to evaluate the model fit

```{r, echo=TRUE}
numberNA2 = NULL
for (i in 1:160) {
        numberNA2[i]<-sum(is.na(validation[,i]))
        numberNA2
}
df2<-data.frame(numberNA2)
df2$col<-rownames(df2)
selVar2<-df2[df2$numberNA2<dim(validation)[1]*0.975, ]
validationNew<-validation[,as.numeric(selVar2$col)]
confusionMatrix(validationNew$class,predict(modFit, validationNew) )
```

The accuracy rate for my validation dataset is about 66.15%. 
